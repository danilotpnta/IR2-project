#!/bin/bash

#SBATCH --partition=gpu_h100
#SBATCH --gpus=1

#SBATCH --job-name=InPars-GEN
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=16
#SBATCH --time=10:59:00
#SBATCH --output=/scratch-shared/scur2850/logs/train_%A.out

#SBATCH --ear=on
#SBATCH --ear-policy=monitoring
#SBATCH --ear-verbose=1

date
export HF_HOME="/scratch-shared/$USER"
export GPU_COUNT=$(nvidia-smi --list-gpus | wc -l)

# make sure the correct modules are used and that the virtual environment is active
PROJECT_ROOT=/home/$USER/IR2-project
source $PROJECT_ROOT/scripts/snellius_setup.sh
setup $PROJECT_ROOT
cd $PROJECT_ROOT

STEP_NAME=train
# fill in template variables
LOG_PATH="/scratch-shared/scur2850/logs/$STEP_NAME"_"$SLURM_JOB_ID.out"
SCRIPT_PATH=$PROJECT_ROOT/scripts/step_by_step/$STEP_NAME.job

# NOTE: make sure this is consistent over all steps
RUN_NAME=InPars-MiniLM-L-6-v2
OUTPUT_DIR=/scratch-shared/scur2850/inpars/$RUN_NAME
mkdir -p $OUTPUT_DIR

# ___________________
# |    ARGUMENTS    |
# ___________________

DATASET=scifact # trec-covid
## ExtraArguments
TRIPLES=$OUTPUT_DIR/$DATASET-triples.jsonl
MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2 # paper uses castorini/monot5-3b-msmarco-10k
OUTPUT=$OUTPUT_DIR/$DATASET/reranker
## TrainingArguments
MAX_DOC_LENGTH=300 # default
MAX_STEPS=256 # paper uses 156
#BATCH_SIZE=64
AUTO_BATCH=--auto_find_batch_size
DTYPE=--fp16 # --bf16 --fp16, --int8 or ""
RESUME=false # $OUTPUT or false
# __________________
# |      EXEC      |
# __________________

# rest of the script
srun python -m inpars.train \
--triples $TRIPLES \
--base_model $MODEL \
--max_doc_length $MAX_DOC_LENGTH \
--output_dir $OUTPUT \
--max_steps $MAX_STEPS \
--resume_from_checkpoint $RESUME \
$AUTO_BATCH \
$DTYPE \

mkdir -p $OUTPUT_DIR/job_${STEP_NAME}_${SLURM_JOB_ID}
# copy the script and the logs to the output directory.
# This should be done regardless of the success of the job
echo "Copying logs and script to $OUTPUT_DIR/job_${STEP_NAME}_${SLURM_JOB_ID}"
cp $SCRIPT_PATH $LOG_PATH $OUTPUT_DIR/job_${STEP_NAME}_${SLURM_JOB_ID}
