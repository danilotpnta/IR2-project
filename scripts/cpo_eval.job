#!/bin/bash

#SBATCH --partition=gpu_h100
#SBATCH --gpus=1

#SBATCH --job-name=CPO_EVAL
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=16
#SBATCH --time=23:59:00
#SBATCH --output=/scratch-shared/scur2850/logs/cpo_eval_%A.out

# SBATCH --ear=on
# SBATCH --ear-policy=monitoring
# SBATCH --ear-verbose=1

date
export HF_HOME="/scratch-shared/$USER"
export GPU_COUNT=$(nvidia-smi --list-gpus | wc -l)

# make sure the correct modules are used and that the virtual environment is active
PROJECT_ROOT=/home/$USER/IR2-project
source $PROJECT_ROOT/scripts/snellius_setup.sh
setup $PROJECT_ROOT
cd $PROJECT_ROOT

# parameters
RUN_NAME=cpo_llama8b_sci_arg_cf_cov_tou_vllm-16bit-merged # change this for each run
# RUN_NAME=baseline_llama8b # name of the baseline run
DATASET_PATH=(/scratch-shared/scur2577/cpo_dataset/arguana/llmt_preference_Meta-Llama-3.1-8B-Instruct-FP8_10000.json /scratch-shared/scur2577/cpo_dataset/climate_fever/llmt_preference_Meta-Llama-3.1-8B-Instruct-FP8_10000.json /scratch-shared/scur2577/cpo_dataset/scidocs/llmt_preference_Meta-Llama-3.1-8B-Instruct-FP8_10000.json /scratch-shared/scur2577/cpo_dataset/trec_covid/llmt_preference_Meta-Llama-3.1-8B-Instruct-FP8_10000.json /scratch-shared/scur2577/cpo_dataset/webis_touche2020/llmt_preference_Meta-Llama-3.1-8B-Instruct-FP8_10000.json)
MODEL_NAME=mjhamar/Meta-Llama-3.1-8B-Instruct-cpo-beir
#MODEL_NAME=meta-llama/Meta-Llama-3.1-8B-Instruct
#ADAPTER_NAME=mjhamar/Meta-Llama-3.1-8B-Instruct-cpo-beir
OUTPUT_DIR=/scratch-shared/scur2850/cpo_eval_$RUN_NAME
# QUERY_EVAL_PATH=/scratch-shared/scur2850/cpo_dataset
BATCH_SIZE=32

# rest of the script
python -m inpars.cpo_eval \
--dataset "${DATASET_PATH[@]}" \
--model_name $MODEL_NAME \
--output_dir $OUTPUT_DIR \
--query_eval_path $QUERY_EVAL_PATH \
--batch_size $BATCH_SIZE \
--run_name $RUN_NAME \
--use_wandb \
--use_vllm
