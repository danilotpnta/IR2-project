#!/bin/bash

#SBATCH --partition=gpu_h100
#SBATCH --gpus=1

#SBATCH --job-name=CPO_EVAL
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=16
#SBATCH --time=00:15:00
#SBATCH --output=/scratch-shared/scur2850/logs/cpo_eval_%A.out

#SBATCH --ear=on
#SBATCH --ear-policy=monitoring
#SBATCH --ear-verbose=1

date
export HF_HOME="/scratch-shared/$USER"
export GPU_COUNT=$(nvidia-smi --list-gpus | wc -l)

# make sure the correct modules are used and that the virtual environment is active
PROJECT_ROOT=/home/$USER/IR2-project
source $PROJECT_ROOT/scripts/snellius_setup.sh
setup $PROJECT_ROOT
cd $PROJECT_ROOT

# parameters
RUN_NAME=cpo_llama8b_msmarco100k_ckpt130500 # change this for each run
# RUN_NAME=baseline_llama8b # name of the baseline run
DATASET_PATH=/scratch-shared/scur2850/cpo_output/dataset100/llmt_cpo_inparsplus
MODEL_NAME=/scratch-shared/scur2850/cpo_output/model/checkpoint-130500/
OUTPUT_DIR=/scratch-shared/scur2850/cpo_eval_$RUN_NAME
QUERY_EVAL_PATH=/scratch-shared/scur2850/cpo_dataset
BATCH_SIZE=32

# rest of the script
srun python -m inpars.cpo_eval \
--dataset $DATASET_PATH \
--model_name $MODEL_NAME \
--output_dir $OUTPUT_DIR \
--query_eval_path $QUERY_EVAL_PATH \
--batch_size $BATCH_SIZE \
--run_name $RUN_NAME \
--use_vllm \
--use_wandb
