#!/bin/bash

#SBATCH --partition=gpu_h100
#SBATCH --gpus=1

#SBATCH --job-name=quantize
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=16
#SBATCH --time=23:59:00
#SBATCH --mem=180GB
#SBATCH --output=/scratch-shared/scur2577/runs/%A.out
#/SBATCH --error=/scratch-shared/scur2577/runs/%a.err

#SBATCH --ear=on
#SBATCH --ear-policy=monitoring
#SBATCH --ear-verbose=1
date

# make sure the correct modules are used and that the virtual environment is active
PROJECT_ROOT=/home/$USER/IR2-project
source $PROJECT_ROOT/scripts/snellius_setup.sh
setup $PROJECT_ROOT

module load 2023
module load 2024
module load OpenMPI/4.1.5-NVHPC-24.5-CUDA-12.1.1
# It's a mess, but you may need some of these to build the wheels
module load Cython/3.0.10-GCCcore-13.3.0
module load Clang/16.0.6-GCCcore-12.3.0
module load iimpi/2023a
# uv venv .venv3.10 -p 3.10

source $PROJECT_ROOT/.venv3.10/bin/activate

# git clone https://github.com/NVIDIA/TensorRT-LLM
# cd TensorRT-LLM/examples/quantization
# Note, some reqs throw errors, but you can remove nemo*[all]* and you can probably build fine
uv pip install -r requirements.txt

export MODEL_PATH=~/.cache/huggingface/hub/models--nvidia--Llama-3.1-Nemotron-70B-Instruct-HF/
export OUTPUT_PATH=/scratch-shared/scur2577/bigmodel
mkdir -p $OUTPUT_PATH

cd $PROJECT_ROOT/TensorRT-LLM/examples/quantization
echo "Begin quantization -gl"
python quantize.py --model_dir $MODEL_PATH  --autoq_format fp8,int4_awq,w4a8_awq  --output_dir $OUTPUT_PATH --auto_quantize_bits 5 --tp_size 2 --use_fp8_context_fmha